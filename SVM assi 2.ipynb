{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1119a20d-90fd-488b-812c-d19a5d9c3b12",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f2950-511f-4e66-9b31-ed356f922306",
   "metadata": {},
   "source": [
    "Polynomial Functions and Kernel Functions: A Connection\n",
    "Polynomial functions are mathematical expressions involving variables raised to non-negative integer powers. They are used in various fields, including machine learning.\n",
    "\n",
    "Kernel functions, on the other hand, are used in machine learning algorithms to compute similarity or distance between data points without explicitly mapping them to a higher-dimensional space.\n",
    "\n",
    "The Connection: Polynomial Kernel\n",
    "The key link between these two concepts lies in the polynomial kernel.\n",
    "\n",
    "Polynomial kernel is a specific type of kernel function that implicitly maps data points to a higher-dimensional space where polynomial functions are used to compute similarity.\n",
    "It calculates the similarity between two data points based on the dot product of their corresponding feature vectors, raised to a specified power (degree).\n",
    "By using a polynomial kernel, machine learning algorithms can effectively capture non-linear relationships in the data without explicitly performing the computationally expensive transformation to a higher-dimensional space.\n",
    "In essence, polynomial functions form the basis for the polynomial kernel, which is a tool for handling non-linearity in machine learning.\n",
    "\n",
    "Example:\n",
    "Consider a dataset that is not linearly separable. By applying a polynomial kernel to an SVM, we implicitly map the data to a higher-dimensional space where it might become linearly separable. This allows the SVM to find a decision boundary that effectively separates the data.\n",
    "\n",
    "Key points to remember:\n",
    "\n",
    "Polynomial kernels are just one type of kernel function.\n",
    "Other kernel functions like RBF (Radial Basis Function) kernel exist, which use different mathematical formulations.\n",
    "The choice of kernel function depends on the nature of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f763a-e697-4b43-bed2-cbf93f1b84a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de76224-6465-499c-adf2-605b82c5182e",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b60d4dcc-e265-457f-a9b3-59cfd8fe1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c34f9eb-7706-4eef-b88b-f2ced6019843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "X=iris.data[:,:2]\n",
    "y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2005dd54-683f-4a01-b72f-76fbe8297b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set and a testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03dc8e0-f6c7-4df0-a74b-bce75abb1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVM model with a polynomial kernel\n",
    "svm_model = SVC(kernel='poly', degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0972d281-747d-40e4-96de-e3817ed75fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6edcab13-51fe-4aac-a8b1-83f0f853b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9cfad7-3931-4a6e-9a0d-10b842c88eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 2, 0, 2, 0, 2, 2, 1, 1, 2, 1, 2, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 2, 0, 0, 2, 1, 0, 2, 1, 0, 1, 2, 1, 0, 1, 1, 1, 2, 0, 2, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8923880b-7f65-44a7-a834-5bffb9b2033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d78df-830e-48bf-b684-7abc3f20893e",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Import necessary libraries: We import numpy for numerical operations, SVC from sklearn.svm for the SVM model, train_test_split for splitting data, load_iris for loading the iris dataset, and accuracy_score for evaluating the model.\n",
    "Load the dataset: We load the iris dataset using load_iris().\n",
    "Split the data: We split the data into training and testing sets using train_test_split.\n",
    "Create SVM model: We create an SVM model using SVC(kernel='poly', degree=3). The kernel='poly' specifies a polynomial kernel, and degree=3 sets the degree of the polynomial. You can adjust the degree parameter to control the complexity of the model.\n",
    "Train the model: We train the model using the fit() method.\n",
    "Make predictions: We make predictions on the testing set using the predict() method.\n",
    "Evaluate performance: We calculate the accuracy of the model using accuracy_score.\n",
    "Key points:\n",
    "The degree parameter in the polynomial kernel controls the complexity of the model. A higher degree can lead to overfitting if not carefully tuned.\n",
    "You can experiment with different kernel parameters and hyperparameters to optimize the model's performance.\n",
    "Consider using cross-validation to get a more reliable estimate of the model's performance.\n",
    "By following these steps and understanding the parameters involved, you can effectively implement SVM with a polynomial kernel in Python using Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c2cf6-483c-4867-ad8b-5f1071b2880e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f07f431-0698-4e7b-93e3-a14a03eb11b0",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff08c64-0d0f-4b72-ad7b-af14f28d5451",
   "metadata": {},
   "source": [
    "Epsilon and Support Vectors in SVR\n",
    "Understanding Epsilon\n",
    "In Support Vector Regression (SVR), epsilon defines the size of the tube around the regression function where no penalty is incurred. Data points within this tube are considered correct predictions and do not contribute to the loss function.\n",
    "\n",
    "Impact of Epsilon on Support Vectors\n",
    "Increasing epsilon: When you increase the value of epsilon, the size of the tube around the regression function increases. This means more data points will fall within this tube, and fewer points will be considered outliers or errors. As a result, the number of support vectors (data points that influence the model) will decrease.\n",
    "\n",
    "Decreasing epsilon: Conversely, decreasing epsilon makes the tube narrower. More data points will be considered outliers, and the model will try to fit them more closely. This leads to an increase in the number of support vectors.\n",
    "\n",
    "Key Points\n",
    "Epsilon is a crucial hyperparameter in SVR that controls the model's tolerance for errors.\n",
    "A larger epsilon leads to a simpler model with fewer support vectors, while a smaller epsilon results in a more complex model with more support vectors.\n",
    "The optimal value of epsilon depends on the specific dataset and problem at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea968eb6-97bd-4b49-a2c6-a515abb10d77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94811992-a585-40a4-ba49-9ba52dac0462",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514a484-845b-4975-9dcf-c3bf1ea2bb35",
   "metadata": {},
   "source": [
    "Impact of Parameters on SVR Performance\n",
    "Kernel Function\n",
    "The kernel function determines the similarity measure between data points. Common kernels include:\n",
    "\n",
    "Linear: Suitable for linearly separable data.\n",
    "Polynomial: Captures polynomial relationships between features.\n",
    "RBF (Radial Basis Function): Effective for non-linear relationships and works well in many cases.\n",
    "Sigmoid: Similar to logistic regression, but less commonly used.\n",
    "When to increase/decrease:\n",
    "\n",
    "Linear: Use when you believe the data is linearly separable or for computational efficiency on large datasets.\n",
    "Polynomial: Experiment with different degrees to capture complex patterns. Increase the degree for more complex relationships, but beware of overfitting.\n",
    "RBF: Generally a good starting point. Adjust gamma to control the influence of data points.\n",
    "Sigmoid: Consider if you have prior knowledge of logistic regression-like behavior.\n",
    "C Parameter\n",
    "C controls the trade-off between maximizing the margin and minimizing the training error. A higher C implies a stricter margin, leading to less tolerance for errors but potentially overfitting. A lower C allows for a wider margin but might underfit.\n",
    "\n",
    "When to increase/decrease:\n",
    "\n",
    "Increase C: When you want a more complex model and are willing to tolerate some overfitting.\n",
    "Decrease C: When you prioritize generalization and want to avoid overfitting.\n",
    "Epsilon Parameter\n",
    "Epsilon defines the size of the tube around the regression function where no penalty is incurred. A larger epsilon allows for more tolerance in the prediction, reducing the number of support vectors.\n",
    "\n",
    "When to increase/decrease:\n",
    "\n",
    "Increase epsilon: When you want a simpler model with fewer support vectors and are willing to accept a larger error margin.\n",
    "Decrease epsilon: When you require higher precision and are willing to deal with a more complex model.\n",
    "Gamma Parameter\n",
    "Gamma is used in kernel functions like RBF and polynomial. It controls the influence of data points. A higher gamma means a smaller influence of data points, resulting in a more complex decision boundary. A lower gamma has the opposite effect.\n",
    "\n",
    "When to increase/decrease:\n",
    "\n",
    "Increase gamma: When you believe the decision boundary is complex and data points have a strong local influence.\n",
    "Decrease gamma: When you believe the decision boundary is simpler and data points have a broader influence.\n",
    "Key Considerations\n",
    "The optimal parameter values often depend on the specific dataset and problem.\n",
    "Grid search or randomized search cross-validation can be used to find the best combination of parameters.\n",
    "Feature scaling can significantly impact SVR performance.\n",
    "Overfitting is a common issue, so it's essential to balance model complexity with generalization.\n",
    "By carefully considering these parameters and experimenting with different values, you can achieve optimal performance with SVR for your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08706b61-6fc2-4e4e-9bfb-b440b825a601",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eda6a500-15eb-45be-b717-38a79bf6849f",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Import the necessary libraries and load the dataset\n",
    "Split the dataset into training and testing sets\n",
    "Preprocess the data using any technique of your choice (e.g. scaling, normalization.\n",
    "Create an instance of the SVC classifier and train it on the training data\n",
    "Use the trained classifier to predict the labels of the testing data\n",
    "Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score)\n",
    "Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance.\n",
    "Train the tuned classifier on the entire dataset\n",
    "Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52aed13-8a50-4892-878a-9acabcdde648",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0f92312-129b-4e66-8514-c94af709007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923443e5-bdf3-4425-9d1e-b7bfd42e4333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8912ef82-f5d6-4eeb-a879-1471d7670a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61e088-6717-4c03-b6f8-5b8419a4e263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
