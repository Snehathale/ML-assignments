{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048aaec4-44cb-40c1-b3cc-fb279c653af6",
   "metadata": {},
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72818aa2-3307-4516-9fc4-4958e3cfb73e",
   "metadata": {},
   "source": [
    "A decision tree classifier is a machine learning algorithm that uses a tree-like model to classify data. Here's how it works:\n",
    "\n",
    "Structure:\n",
    "\n",
    "Imagine a flowchart shaped like a tree. The tree consists of three main parts:\n",
    "\n",
    "Nodes: These are decision points where the algorithm asks a question about a specific feature of the data. There are two types:\n",
    "Internal nodes: These represent the questions and further branch out based on the answer.\n",
    "Leaf nodes: These are the final destinations, representing the predicted class label (in classification).\n",
    "Building the Tree:\n",
    "\n",
    "Start with the entire dataset at the root node.\n",
    "Choose the best attribute: The algorithm uses a metric like Gini impurity or information gain to identify the feature that best splits the data into distinct classes.\n",
    "Split the data: Based on the chosen feature and its value, the data is divided into subsets that go down different branches.\n",
    "Repeat: Steps 2 and 3 are repeated for each new subset created, creating further branches and sub-trees. This continues until a stopping criteria is met, like all data points in a branch belong to the same class, or a maximum depth is reached.\n",
    "Making Predictions:\n",
    "\n",
    "Start at the root node.\n",
    "Answer the question: Based on the value of the feature in the new data point you want to classify, you traverse the branch corresponding to the answer.\n",
    "Traverse the tree: Keep following the branches based on the data point's feature values until you reach a leaf node.\n",
    "Prediction: The class label associated with the leaf node becomes the predicted class for the new data point.\n",
    "Essentially, the decision tree acts like a series of yes/no questions that lead you to the most probable class for a new data point.\n",
    "\n",
    "Advantages of Decision Trees:\n",
    "\n",
    "Easy to understand and interpret due to their tree-like structure.\n",
    "Can handle high-dimensional data.\n",
    "No need for feature scaling.\n",
    "Disadvantages of Decision Trees:\n",
    "\n",
    "Prone to overfitting if not carefully grown.\n",
    "Sensitive to small changes in the data.\n",
    "I hope this explanation clarifies how decision tree classifiers work!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1412cbf-fee5-432b-aaef-a303280ae570",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "492a05d1-d2f4-4581-b22a-423d5a5e6cb3",
   "metadata": {},
   "source": [
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6161cf-939e-4439-9582-821ba5c40895",
   "metadata": {},
   "source": [
    "The mathematical core of decision tree classification revolves around choosing the best split at each node to maximize the purity (separability) of the data. Here's a breakdown:\n",
    "\n",
    "1. Measuring Impurity:\n",
    "\n",
    "We need a way to quantify how mixed up (impure) the data is at a particular node. Two common metrics are used:\n",
    "\n",
    "* **Entropy:** This comes from information theory and measures the uncertainty associated with the class labels at a node. A perfectly homogeneous node (all data points belong to one class) has an entropy of 0, while a completely mixed node has an entropy of 1 (maximum uncertainty).\n",
    "\n",
    "* **Gini Impurity:** This metric calculates the probability of misclassifying a data point if a random guess were made based on the class distribution at the node. A value of 0 indicates perfect purity, while 1 denotes maximum impurity.\n",
    "2. Choosing the Best Split:\n",
    "\n",
    "Now, we need a way to decide which feature (attribute) best separates the data into distinct classes. This is where the concept of Information Gain comes in.\n",
    "\n",
    "Information Gain: It measures the decrease in uncertainty (entropy or Gini impurity) after splitting the data based on a particular feature.\n",
    "Here's the intuition: Imagine the starting entropy (or Gini impurity) represents the overall uncertainty about the class labels in the data. After splitting on a feature, we calculate the weighted average impurity of the resulting child nodes. Information Gain tells us how much \"impurity\" we've reduced by making this split.\n",
    "\n",
    "Steps for Choosing the Best Split:\n",
    "\n",
    "For each feature:\n",
    "Calculate the entropy/Gini impurity after splitting the data based on that feature's values (resulting in child nodes).\n",
    "Weight the impurity of each child node by the proportion of data points it contains.\n",
    "Calculate the average weighted impurity.\n",
    "Choose the feature that leads to the highest decrease in the initial impurity (highest information gain).\n",
    "3. Building the Tree:\n",
    "\n",
    "The algorithm iteratively repeats steps 1 and 2 for each new node, selecting the feature with the highest information gain and splitting the data accordingly. This process continues until a stopping criterion is met, like:\n",
    "\n",
    "All data points in a node belong to the same class (perfect purity).\n",
    "Reaching a maximum depth for the tree (to avoid overfitting).\n",
    "Essentially, the decision tree algorithm uses information gain to greedily find the splits that most effectively separate the data based on their class labels.\n",
    "\n",
    "Additional Notes:\n",
    "\n",
    "There are other splitting criteria besides information gain, like Gini index variance.\n",
    "The calculations for entropy and Gini impurity involve logarithms and probabilities based on the class distribution at each node.\n",
    "This explanation provides a basic understanding of the mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6139962d-4139-401f-812c-0d594dfc5c47",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9707b0df-26da-4d4c-80f6-117780022925",
   "metadata": {},
   "source": [
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661607f-6478-4b60-8baf-d292e8973485",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Binary classification problems involve classifying data points into exactly two categories. Decision tree classifiers excel at tackling such problems due to their inherent structure. Here's how:\n",
    "\n",
    "Scenario:\n",
    "\n",
    "Imagine you have a dataset containing emails, and you want to classify them as spam or not spam (a binary classification task). Each email has features like sender address, keywords in the subject line, and presence of attachments.\n",
    "\n",
    "Building the Tree:\n",
    "\n",
    "Start with the root node: The entire dataset (all emails) resides here.\n",
    "Choose the best split: Use information gain (or Gini impurity) to determine which feature (e.g., presence of specific keywords) best separates the emails into spam and not spam.\n",
    "Split the data: Create two branches from the root node, one for emails containing the chosen keyword (potential spam) and another for emails without it (potential not spam).\n",
    "Repeat recursively: Apply steps 2 and 3 to each new branch (subtree). For example, in the \"potential spam\" branch, you might analyze sender addresses to further refine spam classification.\n",
    "Stopping criteria: Stop growing the tree when:\n",
    "All emails in a branch belong to the same class (spam or not spam).\n",
    "A maximum depth for the tree is reached (to prevent overfitting).\n",
    "Making Predictions:\n",
    "\n",
    "New email arrives: For a new email to classify, you start at the root node.\n",
    "Traverse the tree: Based on the email's features (e.g., presence of the keyword identified earlier), you follow the corresponding branch.\n",
    "Reach a leaf node: The class label associated with the leaf node becomes the predicted class (spam or not spam) for the new email.\n",
    "Advantages for Binary Classification:\n",
    "\n",
    "Clear decision boundaries: The tree structure provides a clear visualization of the decision rules used for classification. You can see how specific features lead to a spam or not spam classification.\n",
    "Interpretability: It's easier to understand the logic behind the decision tree's predictions compared to some other machine learning models.\n",
    "Efficient handling of irrelevant features: The algorithm naturally ignores features that don't contribute to effective splitting, focusing on the most relevant ones for spam/not spam classification.\n",
    "In essence, decision trees provide a step-by-step, rule-based approach to classifying data points into two categories in a binary classification problem. This makes them a popular choice for tasks where interpretability and clear decision boundaries are important."
   ]
  },
  {
   "cell_type": "raw",
   "id": "30f5ce88-2434-4717-8d7d-2c2eaa9cea50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2f4a072-a058-4ba8-bc69-9797f9ab1e94",
   "metadata": {},
   "source": [
    "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61b7c6-91fd-4305-91a2-2c7300fd3d6b",
   "metadata": {},
   "source": [
    "The geometric intuition behind decision tree classification revolves around creating hyperplane splits in a multidimensional feature space to separate data points belonging to different classes.\n",
    "\n",
    "Imagine each data point in your dataset represented by a point in a space with as many dimensions as there are features. For example, if you have a dataset with features like \"petal length\" and \"petal width\" of Iris flowers, you'd have a two-dimensional space.\n",
    "\n",
    "Building the Tree Geometrically:\n",
    "\n",
    "Start at the root: All data points are clustered together at the root, representing the entire dataset in this feature space.\n",
    "\n",
    "Splitting with Hyperplanes:  The decision tree algorithm chooses a feature and a specific value of that feature to create a split. This translates to creating a hyperplane (a flat plane in higher dimensions) that divides the space into two regions. Data points on one side of the hyperplane have a value for the chosen feature less than the split value, while those on the other side have a value greater than the split.\n",
    "\n",
    "In our flower example, the algorithm might choose \"petal length\" and a value of 5 cm. This creates a hyperplane perpendicular to the \"petal length\" axis at the 5 cm mark.\n",
    "Recursive Splits:  This process of creating hyperplane splits is repeated recursively for each new branch of the tree. At each node, a new feature and split value are chosen to further refine the separation between classes.\n",
    "\n",
    "Imagine subsequent splits based on \"petal width\" in specific regions of the feature space created by the initial \"petal length\" split.\n",
    "Predictions with Hyperplanes:\n",
    "\n",
    "To predict the class of a new data point:\n",
    "\n",
    "Project the point:  Imagine placing the new data point in the same feature space.\n",
    "\n",
    "Traverse the tree based on hyperplane intersections:  Follow the branches of the tree based on which side of the hyperplanes in the space the new data point falls on. Each branch represents a decision based on a feature value.\n",
    "\n",
    "Leaf node prediction:  Once you reach a leaf node (terminal point of the tree), the class label associated with that leaf node becomes the predicted class for the new data point.\n",
    "\n",
    "Essentially, the decision tree carves out specific regions in the feature space using hyperplanes. Each region represents a combination of feature values that leads to a specific class prediction.  This geometric view helps visualize how the decision tree progressively refines the separation of classes as it grows.\n",
    "\n",
    "Limitations of Geometric Intuition:\n",
    "\n",
    "This explanation works well for lower-dimensional feature spaces (2 or 3 dimensions). Visualizing hyperplanes in spaces with many dimensions becomes challenging.\n",
    "Decision trees with many features create complex decision boundaries that can't be easily visualized with hyperplanes alone.\n",
    "Despite these limitations, the geometric understanding offers a valuable perspective on how decision trees work by separating data points in a feature space based on a series of sequential decisions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "87a118e8-86c4-4891-a639-477d94de4762",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "068ede4a-03e1-4eb5-9d5b-4faaf95a4799",
   "metadata": {},
   "source": [
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a80e4-7dd6-481b-bddf-3e082517d427",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A confusion matrix is a table layout that visually summarizes the performance of a classification model. It compares the actual labels of the data (ground truth) with the labels predicted by the model. Here's a breakdown of its components and how it helps evaluate model performance:\n",
    "\n",
    "Structure:\n",
    "\n",
    "The confusion matrix is a square table with rows and columns representing the actual and predicted classes, respectively. The size of the matrix depends on the number of classes your model predicts. For a binary classification (two classes), it's a 2x2 matrix, while for multi-class problems, it would be an NxN matrix (where N is the number of classes).\n",
    "\n",
    "Elements of the Matrix:\n",
    "\n",
    "True Positive (TP): These are the cases where the model correctly predicted a positive class.\n",
    "True Negative (TN): These are the cases where the model correctly predicted a negative class.\n",
    "False Positive (FP): These are the cases where the model incorrectly predicted a positive class (also known as Type I error).\n",
    "False Negative (FN): These are the cases where the model incorrectly predicted a negative class (also known as Type II error).\n",
    "Evaluating Model Performance:\n",
    "\n",
    "By analyzing the counts in the confusion matrix, you can calculate various metrics to assess the strengths and weaknesses of your classification model. Here are some common metrics:\n",
    "\n",
    "Accuracy: Overall percentage of correct predictions (TP + TN) / (Total number of predictions).\n",
    "Precision: Proportion of predicted positives that were actually positive (TP / (TP + FP)).\n",
    "Recall: Proportion of actual positives that were correctly identified (TP / (TP + FN)).\n",
    "Confusion matrix benefits go beyond simple accuracy:\n",
    "\n",
    "Identifying Class Imbalance: If the distribution of classes in your data is uneven (e.g., many negative examples and few positive examples), a high overall accuracy might not tell the whole story. The confusion matrix can reveal if the model struggles with the minority class (high FN for that class).\n",
    "Understanding Errors: It helps pinpoint where the model is making the most errors (high FP or FN for a specific class). This can guide further investigation and model improvement.\n",
    "In essence, the confusion matrix provides a clear and concise way to analyze how well your classification model performs on different classes, offering valuable insights beyond a basic accuracy metric."
   ]
  },
  {
   "cell_type": "raw",
   "id": "00ec015a-f47b-4670-b178-329cd92d9d35",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be653664-eeff-419e-9682-d36be6a5055d",
   "metadata": {},
   "source": [
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "calculated from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139fe2f4-14e7-4ab9-a914-7f8e347e5970",
   "metadata": {},
   "source": [
    "Example: Spam Classification\n",
    "Let's consider a confusion matrix for a spam classification model:\n",
    "\n",
    "Prediction\tSpam (Actual Positive)\tNot Spam (Actual Negative)\tTotal\n",
    "Spam (Predicted Positive)\tTP (True Positive) = 10\tFP (False Positive) = 5\t15\n",
    "Not Spam (Predicted Negative)\tFN (False Negative) = 2\tTN (True Negative) = 8\t10\n",
    "Total\t\t\t25\n",
    "\n",
    "Export to Sheets\n",
    "Explanation:\n",
    "\n",
    "Out of 25 emails, the model correctly classified 10 spams (TP) and 8 non-spams (TN).\n",
    "It incorrectly classified 5 non-spams as spam (FP) and missed 2 actual spams (FN).\n",
    "Calculating Metrics:\n",
    "\n",
    "Precision:\n",
    "\n",
    "Precision measures the proportion of predicted positive cases that were actually positive.\n",
    "Precision = TP / (TP + FP) = 10 / (10 + 5) = 0.66 (or 66%)\n",
    "Recall:\n",
    "\n",
    "Recall measures the proportion of actual positive cases that were correctly identified.\n",
    "Recall = TP / (TP + FN) = 10 / (10 + 2) = 0.83 (or 83%)\n",
    "F1 Score:\n",
    "\n",
    "F1 score is a harmonic mean between precision and recall, penalizing models that excell in one but not the other.\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "F1 = 2 * (0.66 * 0.83) / (0.66 + 0.83) = 0.73 (or 73%)\n",
    "Interpretation:\n",
    "\n",
    "This model has a decent accuracy (68%, not shown in the matrix) as it makes a fair number of correct classifications.\n",
    "However, the precision (66%) indicates that out of all emails it flagged as spam, only 66% were actually spam (it generates some false positives).\n",
    "The recall (83%) shows the model catches most of the actual spam emails (misses only 2).\n",
    "The F1 score (73%) provides a balanced view, considering both precision and recall.\n",
    "In conclusion, the confusion matrix and the calculated metrics provide a more nuanced understanding of the model's performance beyond just accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "id": "966e964f-7d4e-4e82-9ef4-d92c966e0767",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41ec17b7-c034-4370-8758-e6413f06ffdb",
   "metadata": {},
   "source": [
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
    "explain how this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69caf21-d437-45c9-ac64-571047e0e8ca",
   "metadata": {},
   "source": [
    "Choosing an appropriate evaluation metric is crucial for effectively assessing a classification model's performance. Here's why it matters and how to make an informed decision:\n",
    "\n",
    "Why Metric Choice Matters:\n",
    "\n",
    "Focus on the Right Problem: Different metrics prioritize different aspects of a model's performance. Using the wrong metric can lead to misleading conclusions.\n",
    "For example, focusing solely on accuracy might be insufficient if the cost of misclassifying certain data points is very high.\n",
    "Understanding Model Behavior: Metrics like precision, recall, and F1 score provide insights into how well the model handles specific classes, especially in imbalanced datasets.\n",
    "Informing Model Improvement: The chosen metric should guide efforts to improve the model. For instance, if the model has high recall but low precision, it might be overfitting and generating too many false positives. Tuning the model to improve precision would be the appropriate course of action.\n",
    "Factors to Consider When Choosing a Metric:\n",
    "\n",
    "Problem Nature:\n",
    "Binary vs. Multi-class: For binary problems, accuracy can be a reasonable starting point. In multi-class problems, metrics like F1 score or macro-averaging of precision and recall might be more informative.\n",
    "Cost of Misclassification: If certain misclassifications are more critical than others (e.g., spam detection), consider metrics like cost-sensitive accuracy or Matthews correlation coefficient (MCC).\n",
    "Data Imbalance: If your data has a skewed class distribution (e.g., mostly negative examples), accuracy can be misleading. Use metrics like precision, recall, or F1 score to understand how the model performs on the minority class.\n",
    "Interpretability: Choose metrics that are easy to understand for stakeholders involved in the project.\n",
    "How to Choose the Right Metric:\n",
    "\n",
    "Clearly define the problem and its goals. What kind of classification are you performing? What are the potential consequences of misclassification?\n",
    "Analyze your data. Understand the class distribution and potential imbalances.\n",
    "Consider relevant metrics based on the problem and data characteristics. Research different metrics and their strengths/weaknesses.\n",
    "Evaluate multiple metrics. Don't rely solely on one metric. Use a combination to get a comprehensive picture of the model's performance.\n",
    "Domain knowledge is key. If you have domain expertise, leverage it to understand the true cost of misclassifications and choose metrics that reflect real-world impact.\n",
    "By carefully considering these factors and following a thoughtful selection process, you can choose the most appropriate evaluation metric for your specific classification problem. This will ensure a more accurate assessment of your model's performance and guide effective improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b80a00-9429-4f74-8980-01293149be95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "252135e2-3dcd-42cf-8161-5f6bf1053d41",
   "metadata": {},
   "source": [
    "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e59fc7-c57e-48fa-92be-35d69fe2fae3",
   "metadata": {},
   "source": [
    "Example: Cancer Diagnosis with Biopsy Results\n",
    "Scenario: Imagine a machine learning model that analyzes biopsy images to classify them as cancerous or benign.\n",
    "\n",
    "Why Precision is Crucial:\n",
    "\n",
    "High Cost of False Positives: A false positive occurs when the model incorrectly classifies a benign tumor as cancerous. This leads to unnecessary and invasive procedures like surgery, causing physical and emotional stress to the patient. Additionally, these procedures are expensive and strain healthcare resources.\n",
    "Psychological Impact: A false positive can cause significant anxiety and fear in a patient who is incorrectly diagnosed with cancer.\n",
    "Follow-up Procedures: Even if the false positive is eventually corrected, the patient might still undergo unnecessary biopsies or other procedures to confirm the absence of cancer.\n",
    "Precision Matters Most:\n",
    "\n",
    "In this scenario, precision takes precedence over recall. Here's why:\n",
    "\n",
    "Acceptable Miss Rate: Missing a few actual cancerous tumors (false negatives) can be caught with further testing. While delaying treatment isn't ideal, it's generally less risky than unnecessary surgery on a healthy patient. Early detection methods can be used to catch missed cancers later.\n",
    "Minimizing False Positives: The primary goal is to minimize the number of false positives to avoid unnecessary procedures and psychological distress.\n",
    "Additional Considerations:\n",
    "\n",
    "Doctors would likely use this model as a decision support tool, not a sole basis for diagnosis. They would consider the model's output along with other factors like patient history and symptoms.\n",
    "Other metrics like F1 score might be used alongside precision for a more balanced view.\n",
    "Conclusion:\n",
    "\n",
    "By prioritizing precision in this example, the model aims to reduce the risk of unnecessary harm to patients while acknowledging the importance of catching actual cancers through other means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b96780-7db5-4fce-8110-fcd273791f8a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbb26df0-f899-4f00-9ac5-a9a8c6dd3b35",
   "metadata": {},
   "source": [
    "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052ff97-689d-404e-9f0a-c0904d86bd77",
   "metadata": {},
   "source": [
    "Example: Fraud Detection in Financial Transactions\n",
    "Scenario: Imagine a machine learning model that analyzes credit card transactions to identify potential fraudulent activity.\n",
    "\n",
    "Why Recall is Crucial:\n",
    "\n",
    "High Cost of False Negatives: A false negative occurs when the model fails to identify a fraudulent transaction, allowing it to go through successfully. This leads to financial losses for the bank and the cardholder.\n",
    "Time-Sensitive: Fraudulent transactions often involve stolen credit card information or exploiting vulnerabilities in systems. The faster these transactions are identified, the less damage is done.\n",
    "Recall Matters Most:\n",
    "\n",
    "In this scenario, recall is the most important metric. Here's why:\n",
    "\n",
    "Minimizing Missed Fraud: Even a small number of missed fraudulent transactions (false negatives) can result in significant financial losses. Early detection and prevention are crucial.\n",
    "Acceptable False Positives: While some legitimate transactions might be flagged for review due to false positives, these can be investigated further without causing immediate harm. The inconvenience of a temporary block on a legitimate transaction is less significant than a successful fraudulent one.\n",
    "Additional Considerations:\n",
    "\n",
    "Banks typically implement a layered security approach. This model might be used to flag suspicious transactions for further manual review by fraud analysts, who can make the final decision.\n",
    "Metrics like precision can still be monitored to avoid an excessive number of false positives that overwhelm analysts.\n",
    "Conclusion:\n",
    "\n",
    "By prioritizing recall in this example, the model aims to catch as much fraudulent activity as possible, even if it means some legitimate transactions are flagged for review. This approach minimizes financial losses and protects cardholders from unauthorized charges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f42b2-a836-485a-baef-8e42b1dd2045",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
