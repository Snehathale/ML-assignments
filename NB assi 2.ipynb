{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272488b2-e647-4c18-acb2-0f7e610f3845",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c924f-ae44-4f43-b453-d7522b3863de",
   "metadata": {},
   "source": [
    "Understanding the Problem\n",
    "We're asked to find the probability of an employee being a smoker given that they use the health insurance plan. This is a conditional probability problem.\n",
    "\n",
    "Defining Variables\n",
    "P(H) = Probability of an employee using health insurance = 70% = 0.7\n",
    "P(S|H) = Probability of an employee being a smoker given they use health insurance = 40% = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b88378-dd8c-4762-9796-c159657148ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17c8433a-4bf4-44db-ae93-600a62d056b8",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad615967-a8fd-4f18-8bb6-3ce4a2d299b2",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes vs. Multinomial Naive Bayes\n",
    "Both Bernoulli and Multinomial Naive Bayes are probabilistic classifiers based on Bayes' theorem, but they differ in how they treat features.\n",
    "Bernoulli Naive Bayes\n",
    "Feature representation: Binary (0 or 1).   \n",
    "Suitable for: Text classification where the presence or absence of a word is important, such as spam filtering.   \n",
    "Probability calculation: Calculates the probability of a feature being present or absent given a class.   \n",
    "Multinomial Naive Bayes\n",
    "Feature representation: Count of occurrences.   \n",
    "Suitable for: Text classification where the frequency of words is important, such as document categorization.\n",
    "Probability calculation: Calculates the probability of a feature occurring a specific number of times given a class.\n",
    "In essence:\n",
    "\n",
    "Bernoulli Naive Bayes focuses on whether a feature is present or absent.   \n",
    "Multinomial Naive Bayes focuses on how many times a feature appears.   \n",
    "Which one to use depends on the nature of your data:\n",
    "\n",
    "If your features are binary (e.g., word presence in a document), use Bernoulli Naive Bayes.   \n",
    "If your features are counts (e.g., word frequencies in a document), use Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f332ee-7bdc-4373-9801-aeb2e1c453ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "961f391d-8343-404f-9b66-3677f1e8ebf1",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cacd21-6d2a-4bfa-b757-aeaf8a9d4dc0",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Missing Values\n",
    "Bernoulli Naive Bayes doesn't have a built-in mechanism to handle missing values. This is because it assumes binary features (0 or 1). A missing value doesn't fit into this binary scheme.\n",
    "\n",
    "Common Approaches to Handle Missing Values in Bernoulli Naive Bayes:\n",
    "Ignore Instances with Missing Values:\n",
    "\n",
    "The simplest approach is to remove instances containing missing values from the dataset. However, this can lead to data loss, especially if there are many missing values.\n",
    "Imputation:\n",
    "\n",
    "Replace missing values with a specific value:\n",
    "Zero Imputation: Replace missing values with 0, assuming the feature is absent.\n",
    "Most Frequent Value Imputation: Replace with the most common value for that feature.\n",
    "Mean/Median Imputation: While less common for Bernoulli, you could theoretically replace with the mean or median if the data were treated as continuous.\n",
    "Treat Missing as a Separate Category:\n",
    "\n",
    "Create a new category for missing values, effectively treating it as a new feature. This can be useful if missingness itself is informative.\n",
    "Important Considerations:\n",
    "\n",
    "The choice of handling missing values depends on the nature of the data and the specific problem.\n",
    "Experiment with different methods to find the best approach for your dataset.\n",
    "Be aware that any imputation method introduces bias and might affect the model's performance.\n",
    "In summary, while Bernoulli Naive Bayes doesn't have a built-in method for missing values, careful consideration of imputation techniques can help address this issue.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77e758-0598-4fa6-a837-e37ac9e4975c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50216ec0-ba6d-4710-b8d4-5a4ff92c9ac8",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafff2d9-a18c-49f5-8128-f2331f8d6807",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification.\n",
    "Gaussian Naive Bayes is a variant of Naive Bayes that assumes features follow a normal distribution.\n",
    "\n",
    " While it's often associated with binary classification, it can effectively handle multi-class problems as well.   \n",
    "\n",
    "How it works:\n",
    "\n",
    "Calculate probabilities for each class: For each class, calculate the probability of each feature value given that class, assuming a normal distribution.\n",
    "Apply Bayes' theorem: Use Bayes' theorem to calculate the probability of each class given the observed feature values.\n",
    "Predict the class: Assign the instance to the class with the highest probability.   \n",
    "Key points:\n",
    "\n",
    "Multiple classes: The model calculates probabilities for all possible classes.\n",
    "Normal distribution assumption: The features are assumed to be normally distributed within each class.   \n",
    "Independence assumption: The features are assumed to be independent given the class.   \n",
    "In essence, Gaussian Naive Bayes is versatile and can handle both binary and multi-class classification problems effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed7dd1-98f1-4dce-9178-0506c7ee63b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be229265-3f4a-429f-85e5-8b70304118b3",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f4635-70de-4a05-98ea-d0abdadcd383",
   "metadata": {},
   "source": [
    "Implementing and Comparing Naive Bayes Classifiers for Spam Detection\n",
    "This script implements Bernoulli Naive Bayes (BNB), Multinomial Naive Bayes (MNB), and Gaussian Naive Bayes (GNB) for spam classification on the Spambase dataset using scikit-learn and evaluates their performance.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "scikit-learn\n",
    "pandas\n",
    "Data Download:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Spambase   \n",
    "\n",
    "Script:\n",
    "\n",
    "Python\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score   \n",
    "\n",
    "\n",
    "# Load data\n",
    "data, target = load_svmlight_file(\"spambase.dat\")\n",
    "\n",
    "# Define performance metrics function\n",
    "def evaluate_model(model, X, y):\n",
    "  # Stratified 10-fold cross-validation\n",
    "  cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "  accuracy, precision, recall, f1 = [], [], [], []\n",
    "  for train, test in cv.split(X, y):\n",
    "    model.fit(X[train], y[train])\n",
    "    y_pred = model.predict(X[test])\n",
    "    accuracy.append(accuracy_score(y[test], y_pred))\n",
    "    precision.append(precision_score(y[test], y_pred))\n",
    "    recall.append(recall_score(y[test], y_pred))\n",
    "    f1.append(f1_score(y[test], y_pred))   \n",
    "\n",
    "  return {\"Accuracy\": mean(accuracy), \"Precision\": mean(precision), \"Recall\": mean(recall), \"F1\": mean(f1)}\n",
    "\n",
    "# Evaluate Bernoulli Naive Bayes\n",
    "bnb_results = evaluate_model(BernoulliNB(), data, target)\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(bnb_results)\n",
    "\n",
    "# Evaluate Multinomial Naive Bayes\n",
    "mnb_results = evaluate_model(MultinomialNB(), data, target)\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(mnb_results)\n",
    "\n",
    "# Evaluate Gaussian Naive Bayes\n",
    "gnb_results = evaluate_model(GaussianNB(), data, target)\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(mnb_results)\n",
    "\n",
    "# Function to calculate mean\n",
    "def mean(lst):\n",
    "  return sum(lst) / len(lst)\n",
    "\n",
    "# Discussion\n",
    "# ... (replace with your discussion based on the results)\n",
    "\n",
    "# Conclusion\n",
    "# ... (replace with your conclusion based on the analysis)\n",
    "Use code with caution.\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Replace the \"...\" sections with analysis based on the obtained results. Here's what to consider:\n",
    "\n",
    "Compare accuracy, precision, recall, and F1 scores for each classifier. Which one achieved the highest overall performance?\n",
    "Consider the nature of the data: Does the data align more with binary presence/absence (BNB), word frequencies (MNB), or continuous values (GNB, although less likely in this case)?\n",
    "Limitations of Naive Bayes: Did you observe any limitations, such as the independence assumption not holding true for all features?\n",
    "Conclusion:\n",
    "\n",
    "Summarize your findings based on the performance and discussion. Did a specific Naive Bayes variant excel due to its suitability for the data? Mention any limitations of Naive Bayes observed and suggest potential future work, such as:\n",
    "\n",
    "Hyperparameter tuning for each classifier.\n",
    "Feature engineering to improve performance.\n",
    "Comparison with other classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3b75a-29d6-4cdb-b31c-60f00c5acd08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
