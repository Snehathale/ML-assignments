{
  "metadata": {
    "kernelspec": {
      "name": "",
      "display_name": ""
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1d29b905-3b5e-4f95-89f0-d551ca2cb604",
      "cell_type": "markdown",
      "source": "Q1. What is anomaly detection and what is its purpose?",
      "metadata": {}
    },
    {
      "id": "98c6eb4d-98fc-4b17-a3ba-06887bd76ed8",
      "cell_type": "markdown",
      "source": "Anomaly detection is the process of identifying data points, events, or observations that deviate significantly from the majority of the data and do not conform to an expected pattern or behavior.\n\nPurpose of Anomaly Detection:\nDetect unusual behavior: Spot irregularities that could indicate issues such as fraud, security breaches, or equipment malfunctions.\n\nImprove decision-making: Highlight anomalies to refine models, policies, or operations based on unexpected behavior.\n\nPrevent failures: Enable proactive responses in domains like healthcare, finance, or industrial systems to prevent costly or dangerous failures.\n\nEnsure data quality: Identify and remove corrupted, mislabeled, or erroneous data in datasets",
      "metadata": {}
    },
    {
      "id": "f502e4eb-a77b-42a6-8d7a-11672fa09c7a",
      "cell_type": "markdown",
      "source": "Q2. What are the key challenges in anomaly detection?",
      "metadata": {}
    },
    {
      "id": "9c868162-b4d4-4e58-98ba-efe70daf2c24",
      "cell_type": "markdown",
      "source": "Anomaly detection presents several key challenges:\n\nLack of labeled data: Anomalies are rare and unpredictable, making it difficult to gather enough labeled examples for supervised learning.\n\nData imbalance: Normal instances vastly outnumber anomalies, which can bias models toward predicting everything as normal.\n\nEvolving behavior (concept drift): Patterns of what is considered \"normal\" can change over time, especially in dynamic environments like network traffic or financial markets.\n\nHigh false positive/negative rates: It's challenging to minimize false alarms without missing true anomalies, especially when anomalies are subtle.\n\nContext dependence: What is anomalous in one context may be normal in another. Context-aware detection is often necessary.\n\nScalability: Analyzing large, high-dimensional datasets efficiently while maintaining accuracy is technically demanding.\n\nInterpretability: Even when anomalies are detected, understanding why they are anomalous can be difficult, especially with complex models",
      "metadata": {}
    },
    {
      "id": "949c8916-aab2-4746-8e80-74db26d8d619",
      "cell_type": "markdown",
      "source": "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?",
      "metadata": {}
    },
    {
      "id": "220d0fea-1a49-4122-9d7f-16fc27f27b4a",
      "cell_type": "markdown",
      "source": "Unsupervised anomaly detection and supervised anomaly detection differ mainly in their use of labeled data and learning approach:\n\n1. Supervised Anomaly Detection:\nUses labeled data: Requires a training dataset where each instance is labeled as â€œnormalâ€ or â€œanomalous.â€\n\nLearns patterns explicitly: The model learns to distinguish anomalies based on the examples provided.\n\nPros: Typically more accurate if labeled data is abundant and balanced.\n\nCons: Hard to obtain sufficient labeled anomalies, which are often rare or unknown in advance.\n\n2. Unsupervised Anomaly Detection:\nNo labeled data: Assumes that most data points are normal and that anomalies are rare and different.\n\nFinds patterns autonomously: Identifies outliers based on statistical deviation, clustering, or density-based methods.\n\nPros: Useful when labeled anomalies are unavailable or impractical to gather.\n\nCons: May misclassify rare but normal events as anomalies (and vice versa).",
      "metadata": {}
    },
    {
      "id": "3c58209f-981e-4e5d-956e-6b5a412a189d",
      "cell_type": "markdown",
      "source": "Q4. What are the main categories of anomaly detection algorithms?",
      "metadata": {}
    },
    {
      "id": "8acecd5d-ab50-4b52-a0f3-d7efc0884124",
      "cell_type": "markdown",
      "source": "Anomaly detection algorithms can be grouped into several main categories, based on their underlying approach:\n\n1. Statistical Methods\nAssume data follows a known distribution (e.g., Gaussian).\n\nFlag points that deviate significantly from expected statistical behavior.\n\nExamples: Z-score, Grubbsâ€™ test, Gaussian Mixture Models.\n\n2. Machine Learning-Based Methods\na. Supervised Learning\nTrained on labeled data (normal vs. anomalous).\n\nExamples: Decision Trees, Support Vector Machines (SVM), Neural Networks.\n\nb. Unsupervised Learning\nNo labels; anomalies identified as data points that donâ€™t fit the general structure.\n\nExamples: K-Means, DBSCAN, Isolation Forest, Autoencoders.\n\n3. Proximity-Based Methods\nAssume normal data points are close to their neighbors.\n\nAnomalies are far from other points.\n\nExamples: K-Nearest Neighbors (KNN), Local Outlier Factor (LOF).\n\n4. Reconstruction-Based Methods\nUse models (often neural networks) to reconstruct input; high reconstruction error implies anomaly.\n\nExamples: Autoencoders, PCA (Principal Component Analysis).\n\n5. Deep Learning Methods\nHandle complex, high-dimensional data with deep architectures.\n\nExamples: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs) for anomaly detection.\n\n6. Ensemble Methods\nCombine multiple models to improve robustness.\n\nExamples: Isolation Forest, Random Cut Forest.",
      "metadata": {}
    },
    {
      "id": "02b1b87d-77d6-4e7c-9566-d00519794ca7",
      "cell_type": "markdown",
      "source": "Q5. What are the main assumptions made by distance-based anomaly detection methods?",
      "metadata": {}
    },
    {
      "id": "2c8af14d-8454-4a02-80ba-19f1a1f383bd",
      "cell_type": "markdown",
      "source": "Distance-based anomaly detection methods rely on a few key assumptions to identify anomalies:\n\n1. Normal data is densely clustered:\nThe majority of data points are expected to form dense regions (clusters) in the feature space.\n\n2. Anomalies are far from other points:\nAnomalies lie at a significant distance from their nearest neighbors or clusters, making them stand out.\n\n3. Distance metrics are meaningful:\nEuclidean or other distance measures accurately capture the similarity between data points. This assumes that:\n\nAll features are on comparable scales.\n\nThereâ€™s no irrelevant or noisy feature dominating the distance.\n\n4. Data resides in a low or moderate dimensional space:\nHigh-dimensional data can dilute distance measures (a phenomenon known as the \"curse of dimensionality\"), making it harder to distinguish anomalies.",
      "metadata": {}
    },
    {
      "id": "951ea3db-757e-47ff-836b-7ce02c2d7384",
      "cell_type": "markdown",
      "source": "Q6. How does the LOF algorithm compute anomaly scores?",
      "metadata": {}
    },
    {
      "id": "fb3b5055-658f-4dd9-b309-a135ab4e4142",
      "cell_type": "markdown",
      "source": "he Local Outlier Factor (LOF) algorithm computes anomaly scores by comparing the local density of a data point to that of its neighbors. Here's how it works step-by-step:\n\n1. Compute k-distance:\nFor each point, LOF finds its k-nearest neighbors (based on a distance metric like Euclidean distance).\n\n2. Compute reachability distance:\nFor a point \nğ´\nA and its neighbor \nğµ\nB, the reachability distance is defined as:\n\nreach-dist\nğ‘˜\n(\nğ´\n,\nğµ\n)\n=\nmax\nâ¡\n(\nk-distance\n(\nğµ\n)\n,\ndistance\n(\nğ´\n,\nğµ\n)\n)\nreach-dist \nk\nâ€‹\n (A,B)=max(k-distance(B),distance(A,B))\nThis helps reduce sensitivity to noise and outliers.\n\n3. Compute local reachability density (LRD):\nThe LRD of point \nğ´\nA is the inverse of the average reachability distance from its neighbors:\n\nLRD\n(\nğ´\n)\n=\n(\n1\nâˆ‘\nğµ\nâˆˆ\nğ‘˜\nğ‘\nğ‘\n(\nğ´\n)\nreach-dist\nğ‘˜\n(\nğ´\n,\nğµ\n)\n)\nLRD(A)=( \nâˆ‘ \nBâˆˆkNN(A)\nâ€‹\n reach-dist \nk\nâ€‹\n (A,B)\n1\nâ€‹\n )\n4. Compute LOF score:\nThe LOF score is the ratio of the average LRD of \nğ´\nAâ€™s neighbors to \nğ´\nAâ€™s own LRD:\n\nLOF\n(\nğ´\n)\n=\nâˆ‘\nğµ\nâˆˆ\nğ‘˜\nğ‘\nğ‘\n(\nğ´\n)\nLRD\n(\nğµ\n)\nLRD\n(\nğ´\n)\nâˆ£\nğ‘˜\nğ‘\nğ‘\n(\nğ´\n)\nâˆ£\nLOF(A)= \nâˆ£kNN(A)âˆ£\nâˆ‘ \nBâˆˆkNN(A)\nâ€‹\n  \nLRD(A)\nLRD(B)\nâ€‹\n \nâ€‹\n \nInterpretation:\nLOF â‰ˆ 1 â†’ Point is in a dense region (normal).\n\nLOF > 1 â†’ Point is in a sparser region than its neighbors (potential anomaly).\n\nHigher LOF = more anomalous.",
      "metadata": {}
    },
    {
      "id": "ee4de397-dce1-4e39-9bc4-1c5f93207ca5",
      "cell_type": "markdown",
      "source": "Q7. What are the key parameters of the Isolation Forest algorithm?",
      "metadata": {}
    },
    {
      "id": "f7170d8e-1819-4493-acd6-bf4cc8645df7",
      "cell_type": "markdown",
      "source": "The Isolation Forest algorithm has several key parameters that influence its behavior and performance:\n\n1. n_estimators (Number of Trees):\nNumber of isolation trees to build.\n\nMore trees generally improve accuracy but increase computation time.\n\n2. max_samples:\nNumber of samples to draw to build each tree.\n\nCan be an integer or a float (fraction of the dataset).\n\nSmaller values speed up the model but may reduce precision.\n\n3. max_features:\nNumber of features used to split nodes in each tree.\n\nReducing this can help in high-dimensional settings to avoid overfitting.\n\n4. contamination:\nExpected proportion of anomalies in the data.\n\nUsed to set the decision threshold on anomaly scores.\n\nImportant when predicting whether a point is an outlier or not.\n\n5. random_state:\nSeed for random number generation to ensure reproducibility.\n\nOptional â€“ max_depth:\nMaximum depth of each tree (related to how finely the data gets partitioned).",
      "metadata": {}
    },
    {
      "id": "1070eb67-ea30-46f4-b876-01dbd48423ad",
      "cell_type": "markdown",
      "source": "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\nusing KNN with K=10?",
      "metadata": {}
    },
    {
      "id": "764a37d0-5815-4728-a4ea-cb1871e46163",
      "cell_type": "markdown",
      "source": "To compute an anomaly score using K-Nearest Neighbors (KNN) with K=10, we typically consider the distance to the 10th nearest neighbor or the average distance to the 10 nearest neighbors as the anomaly score.\n\nIn your scenario:\n\nThe data point has only 2 neighbors within a radius of 0.5.\n\nWe're asked to compute an anomaly score using K=10.\n\nKey Insight:\nSince the point has only 2 nearby neighbors, the remaining 8 neighbors (to make up 10) must be much farther away â€” possibly outside the 0.5 radius.\n\nAnomaly Score Interpretation:\nIn KNN-based anomaly detection:\n\nHigher average distance to the K nearest neighbors = higher anomaly score.\n\nThis point has very few close neighbors and many distant ones, so:\n\nIts average distance to its 10 nearest neighbors will be relatively large.\n\nHence, its anomaly score is high, meaning itâ€™s likely an outlier.\n\nTo give a numerical score, weâ€™d need the actual distances to all 10 nearest neighbors. But conceptually:\n\nThe anomaly score is high because the point is isolated from most of its K neighbors.",
      "metadata": {}
    },
    {
      "id": "89e11f7e-82c8-4bec-bebc-c6d39b5ea56b",
      "cell_type": "markdown",
      "source": "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\nanomaly score for a data point that has an average path length of 5.0 compared to the average path\nlength of the trees?",
      "metadata": {}
    },
    {
      "id": "7685b167-548c-41d9-801d-b2e5d6b47b9d",
      "cell_type": "markdown",
      "source": "To calculate the anomaly score in Isolation Forest, we use the following formula:\n\nğ‘ \n(\nğ‘¥\n,\nğ‘›\n)\n=\n2\nâˆ’\nğ¸\n(\nâ„\n(\nğ‘¥\n)\n)\nğ‘\n(\nğ‘›\n)\ns(x,n)=2 \nâˆ’ \nc(n)\nE(h(x))\nâ€‹\n \n \nWhere:\n\nğ‘ \n(\nğ‘¥\n,\nğ‘›\n)\ns(x,n): anomaly score for point \nğ‘¥\nx\n\nğ¸\n(\nâ„\n(\nğ‘¥\n)\n)\nE(h(x)): average path length of point \nğ‘¥\nx across all trees (given as 5.0)\n\nğ‘›\nn: number of data points (given as 3000)\n\nğ‘\n(\nğ‘›\n)\nc(n): average path length of unsuccessful searches in a Binary Search Tree, approximated by:\n\nğ‘\n(\nğ‘›\n)\nâ‰ˆ\n2\nâ‹…\n(\nln\nâ¡\n(\nğ‘›\nâˆ’\n1\n)\n+\nğ›¾\n)\nâˆ’\n2\n(\nğ‘›\nâˆ’\n1\n)\nğ‘›\nc(n)â‰ˆ2â‹…(ln(nâˆ’1)+Î³)âˆ’ \nn\n2(nâˆ’1)\nâ€‹\n \nwhere \nğ›¾\nâ‰ˆ\n0.5772\nÎ³â‰ˆ0.5772 (Euler-Mascheroni constant)\n\nStep 1: Compute \nğ‘\n(\n3000\n)\nc(3000)\nln\nâ¡\n(\n2999\n)\nâ‰ˆ\n8.006\nln(2999)â‰ˆ8.006\nğ‘\n(\n3000\n)\nâ‰ˆ\n2\nâ‹…\n(\n8.006\n+\n0.5772\n)\nâˆ’\n2\nâ‹…\n2999\n3000\nâ‰ˆ\n2\nâ‹…\n8.5832\nâˆ’\n1.999\nâ‰ˆ\n17.1664\nâˆ’\n1.999\n=\n15.1674\nc(3000)â‰ˆ2â‹…(8.006+0.5772)âˆ’ \n3000\n2â‹…2999\nâ€‹\n â‰ˆ2â‹…8.5832âˆ’1.999â‰ˆ17.1664âˆ’1.999=15.1674\nStep 2: Plug into anomaly score formula\nğ‘ \n(\nğ‘¥\n,\n3000\n)\n=\n2\nâˆ’\n5.0\n/\n15.1674\nâ‰ˆ\n2\nâˆ’\n0.3295\nâ‰ˆ\n0.793\ns(x,3000)=2 \nâˆ’5.0/15.1674\n â‰ˆ2 \nâˆ’0.3295\n â‰ˆ0.793\nâœ… Final Answer:\nThe anomaly score is approximately 0.793.\n\nCloser to 1 â†’ more likely an anomaly.\n\nCloser to 0 â†’ more normal.\n\nSo this point is somewhat anomalous, but not extreme.",
      "metadata": {}
    },
    {
      "id": "18d13aa8-72d8-49c9-8ec6-2a5aee470033",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}