{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Q1. You are working on a machine learning project where you have a dataset containing numerical and\ncategorical features. You have identified that some of the features are highly correlated and there are\nmissing values in some of the columns. You want to build a pipeline that automates the feature\nengineering process and handles the missing values.\nDesign a pipeline that includes the following steps\"\nUse an automated feature selection method to identify the important features in the datasetC\nCreate a numerical pipeline that includes the following steps\"\nImpute the missing values in the numerical columns using the mean of the column valuesC\nScale the numerical columns using standardisationC\nCreate a categorical pipeline that includes the following steps\"\nImpute the missing values in the categorical columns using the most frequent value of the columnC\nOne-hot encode the categorical columnsC\nCombine the numerical and categorical pipelines using a ColumnTransformerC\nUse a Random Forest Classifier to build the final modelC\nEvaluate the accuracy of the model on the test dataset.\nNote: Your solution should include code snippets for each step of the pipeline, and a brief explanation of\neach step. You should also provide an interpretation of the results and suggest possible improvements for\nthe pipeline.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "To build a machine learning pipeline that handles feature engineering, missing values, and automated feature selection, we will implement the following steps:\n\nAutomated Feature Selection: We will use SelectKBest with a scoring function such as f_classif (ANOVA F-value) to select important features.\nNumerical Pipeline: Impute missing values using the mean and standardize the numerical columns.\nCategorical Pipeline: Impute missing values with the most frequent category and apply one-hot encoding.\nCombine Pipelines: Use ColumnTransformer to combine the numerical and categorical pipelines.\nModeling: Use a RandomForestClassifier as the final model.\nEvaluation: Evaluate the accuracy of the model on the test dataset.\nHere’s the Python code that implements this pipeline using scikit-learn:\n\nStep 1: Importing Required Libraries\npython\nCopy code\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import make_classification\nStep 2: Generate a Sample Dataset\nFor demonstration purposes, let’s generate a synthetic dataset with numerical and categorical features, including missing values.\n\npython\nCopy code\n# Generate a dataset with 1000 samples, 20 features (15 numerical, 5 categorical)\nX, y = make_classification(n_samples=1000, n_features=15, n_informative=10, n_classes=2, random_state=42)\nX = pd.DataFrame(X, columns=[f'num_{i}' for i in range(15)])\n\n# Create 5 categorical features with random categories\nnp.random.seed(42)\nfor i in range(5):\n    X[f'cat_{i}'] = np.random.choice(['A', 'B', 'C'], size=X.shape[0])\n\n# Introduce missing values\nX.iloc[::10, 0] = np.nan  # missing values in num_0\nX.iloc[::15, 16] = np.nan  # missing values in cat_1\nStep 3: Split Data into Training and Test Sets\npython\nCopy code\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nStep 4: Define Pipelines for Numerical and Categorical Features\nNumerical Pipeline\nImputation: Replace missing values with the mean.\nScaling: Standardize numerical values using StandardScaler.\npython\nCopy code\nnumerical_features = X.select_dtypes(include=['float64', 'int']).columns\nnumerical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\nCategorical Pipeline\nImputation: Replace missing values with the most frequent value.\nEncoding: Use one-hot encoding for categorical features.\npython\nCopy code\ncategorical_features = X.select_dtypes(include=['object']).columns\ncategorical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\nStep 5: Combine the Pipelines Using ColumnTransformer\nWe use ColumnTransformer to apply the numerical pipeline to numerical columns and the categorical pipeline to categorical columns.\n\npython\nCopy code\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_pipeline, numerical_features),\n        ('cat', categorical_pipeline, categorical_features)\n    ]\n)\nStep 6: Feature Selection and Model Pipeline\nWe add a feature selection step (SelectKBest) and then use a RandomForestClassifier for the final classification task.\n\npython\nCopy code\nmodel_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('feature_selection', SelectKBest(score_func=f_classif, k=10)),  # Select 10 best features\n    ('classifier', RandomForestClassifier(random_state=42))\n])\nStep 7: Train and Evaluate the Model\nNow we train the pipeline and evaluate it on the test data.\n\npython\nCopy code\n# Train the model\nmodel_pipeline.fit(X_train, y_train)\n\n# Predict and evaluate on test data\ny_pred = model_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f'Model accuracy: {accuracy * 100:.2f}%')\nExplanation of the Pipeline:\nAutomated Feature Selection: We use SelectKBest with ANOVA F-value to select the most important features. This method keeps the top 10 features that have the strongest relationship with the target variable.\nNumerical Pipeline:\nMissing values are replaced with the mean of the column.\nFeatures are standardized (zero mean, unit variance) for better model performance.\nCategorical Pipeline:\nMissing values are imputed using the most frequent value in each column.\nOne-hot encoding is applied to convert categorical variables into binary variables.\nColumn Transformer: Combines both pipelines so that each step is applied only to its respective type of features.\nRandom Forest Classifier: A versatile and robust ensemble method is used to perform classification.\nResults Interpretation:\nThe final accuracy of the model on the test dataset will be displayed. The results will depend on the synthetic data, but generally, Random Forest tends to perform well in a variety of situations. If the accuracy is low, it suggests potential issues such as:\n\nFeature selection reducing too many relevant features.\nData imbalance or noise.\nPossible Improvements:\nHyperparameter Tuning: Use techniques like GridSearchCV to tune parameters of both the feature selection and the Random Forest.\nMore Advanced Imputation: Instead of simple imputation, you can use iterative imputation techniques (like IterativeImputer) which often provide better estimates for missing data.\nFeature Engineering: You can create new features from existing ones or use techniques like Polynomial Features or interactions between features.\nThis pipeline is modular and can be expanded with more advanced techniques based on the dataset’s complexity.",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\nuse a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\naccuracy.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "To build a pipeline that includes a Random Forest classifier and a Logistic Regression classifier, and then use a Voting Classifier to combine their predictions, we can follow these steps:\n\nLoad the Iris dataset.\nPreprocess the dataset (if necessary).\nCreate pipelines for both Random Forest and Logistic Regression classifiers.\nUse a VotingClassifier to combine their predictions.\nTrain the model using the training data.\nEvaluate the accuracy of the model on the test data.\nLet me walk you through the implementation.\n\npython\nCopy code\n# Step 1: Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\n\n# Step 2: Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Step 3: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Step 4: Create a pipeline for Random Forest and Logistic Regression\n# Logistic Regression pipeline with a scaler\nlogreg_pipeline = Pipeline([\n    ('scaler', StandardScaler()),  # Standardize the data\n    ('logreg', LogisticRegression(random_state=42))\n])\n\n# Random Forest pipeline (scaling is not necessary for tree-based models)\nrf_pipeline = Pipeline([\n    ('rf', RandomForestClassifier(random_state=42))\n])\n\n# Step 5: Combine the classifiers using a VotingClassifier\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('logreg', logreg_pipeline),  # Logistic Regression pipeline\n        ('rf', rf_pipeline)           # Random Forest pipeline\n    ],\n    voting='soft'  # Use soft voting for probabilities\n)\n\n# Step 6: Train the voting classifier\nvoting_clf.fit(X_train, y_train)\n\n# Step 7: Make predictions on the test data\ny_pred = voting_clf.predict(X_test)\n\n# Step 8: Evaluate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy of the Voting Classifier: {accuracy:.4f}\")\nExplanation:\nPipelines are used to chain preprocessing steps and classifiers together.\nThe Logistic Regression pipeline includes a StandardScaler to scale the data since logistic regression is sensitive to feature scaling.\nThe Random Forest pipeline does not require scaling as it is a tree-based method.\nThe VotingClassifier combines the predictions of both classifiers using soft voting, which averages the predicted probabilities from each classifier.\nThe model is trained and evaluated on the Iris dataset.\nYou can run this code in your Python environment, and it will output the accuracy of the voting classifier.",
      "metadata": {}
    }
  ]
}